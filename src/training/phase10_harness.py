#!/usr/bin/env python3
"""
Phase 10: Institutional-Grade Optimization
===========================================
1. Doubly Robust Counterfactual Evaluation
2. Multi-Objective Optimization (Lagrangian)
3. Online Learning Simulation (Rolling Retraining)
4. Competitor Modeling (Win-Prob + Bid Shading)
5. Shadow Deployment Harness (24h Campaign Simulation)
"""
import sys, os, warnings, logging
import numpy as np
from collections import deque
from scipy.sparse import vstack
from scipy import stats as sp_stats

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore")
sys.path.insert(0, os.path.abspath("."))

from sklearn.metrics import roc_auc_score
from src.training.train import load_dataset, build_matrix, FeatureExtractor
from src.bidding.config import config

try:
    import lightgbm as lgb
except ImportError:
    lgb = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prepare_data():
    full_df, global_stats, scaler, stats, top_k_maps = load_dataset()
    fe = FeatureExtractor()
    fe.set_encoding_maps(top_k_maps)
    X_parts, y_ctr_parts, y_cvr_parts = [], [], []
    for start in range(0, len(full_df), 100000):
        chunk = full_df.iloc[start:start+100000]
        c = chunk['click'].values.astype(np.int8)
        v = chunk['conversion'].values.astype(np.int8)
        rows = [r for r in chunk.itertuples(index=False)]
        mat = build_matrix(rows, c, v, global_stats, fe, scaler)
        if mat is not None:
            X_parts.append(mat)
            y_ctr_parts.append(c)
            y_cvr_parts.append(v)
    X = vstack(X_parts)
    y_ctr = np.concatenate(y_ctr_parts)
    y_cvr = np.concatenate(y_cvr_parts)
    prices = full_df['payingprice'].values.astype(float)
    return X, y_ctr, y_cvr, prices, scaler, stats, top_k_maps, full_df

def auc_safe(y, p):
    try: return roc_auc_score(y, p)
    except: return 0.5

BASE_PARAMS = {
    "n_estimators": 300, "learning_rate": 0.03, "num_leaves": 15,
    "max_depth": 4, "min_data_in_leaf": 200, "feature_fraction": 0.7,
    "bagging_fraction": 0.8, "bagging_freq": 5, "lambda_l1": 10.0, "lambda_l2": 10.0,
}

def train_lgbm(X_tr, y_tr, X_va, y_va, params, spw=1.0, init_model=None):
    full_params = dict(objective='binary', metric='auc', verbose=-1, scale_pos_weight=spw, **params)
    m = lgb.LGBMClassifier(**full_params)
    fit_kwargs = dict(eval_set=[(X_va, y_va)], eval_metric='auc',
                      callbacks=[lgb.early_stopping(30, verbose=False)])
    if init_model is not None:
        fit_kwargs['init_model'] = init_model
    m.fit(X_tr, y_tr, **fit_kwargs)
    return m

def calc_roi(clicks, convs, spend):
    value = clicks * config.value_click + convs * config.value_conversion
    return value / max(spend, 1)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. DOUBLY ROBUST COUNTERFACTUAL EVALUATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def task1_counterfactual(X, y_ctr, prices):
    logger.info("=" * 65)
    logger.info("TASK 1: DOUBLY ROBUST COUNTERFACTUAL EVALUATION")
    logger.info("=" * 65)

    n = X.shape[0]
    n_train = int(n * 0.7)
    X_tr, y_tr = X[:n_train], y_ctr[:n_train]
    X_te, y_te = X[n_train:], y_ctr[n_train:]
    prices_te = prices[n_train:]
    spw = (1.0 - y_tr.mean()) / (y_tr.mean() + 1e-6)

    # Train logging policy (old model with weaker params)
    logging_params = dict(BASE_PARAMS)
    logging_params["n_estimators"] = 100
    logging_params["num_leaves"] = 8
    log_model = train_lgbm(X_tr, y_tr, X_te, y_te, logging_params, spw)
    p_log = np.clip(log_model.predict_proba(X_te)[:, 1], 0.01, 0.99)

    # Train target policy (current best model)
    tgt_model = train_lgbm(X_tr, y_tr, X_te, y_te, BASE_PARAMS, spw)
    p_tgt = np.clip(tgt_model.predict_proba(X_te)[:, 1], 0.01, 0.99)

    # Simulate logging actions: logging policy bid on items where p_log > threshold
    threshold = np.median(p_log)
    logged_action = (p_log >= threshold).astype(int)  # 1 = bid, 0 = skip
    target_action = (p_tgt >= threshold).astype(int)

    # Rewards: clicks observed under logging policy
    rewards = y_te.copy().astype(float)
    # Only observe reward where logging policy bid
    rewards[logged_action == 0] = 0

    # Propensity scores
    propensity = np.where(logged_action == 1, p_log, 1 - p_log)
    propensity = np.clip(propensity, 0.05, 0.95)  # Clipping

    # Target policy probability of same action
    target_prob = np.where(logged_action == 1, p_tgt, 1 - p_tgt)

    # Importance weights
    w = target_prob / propensity
    w_clipped = np.clip(w, 0.1, 10.0)  # Clip extreme weights

    # â”€â”€ IPS (Inverse Propensity Scoring) â”€â”€
    ips_estimate = np.mean(w_clipped * rewards)
    ips_var = np.var(w_clipped * rewards) / len(rewards)

    # â”€â”€ SNIPS (Self-Normalized IPS) â”€â”€
    snips_estimate = np.sum(w_clipped * rewards) / np.sum(w_clipped)
    # Variance via delta method
    snips_var = np.var(w_clipped * (rewards - snips_estimate)) / (np.sum(w_clipped) ** 2) * len(rewards)

    # â”€â”€ DR (Doubly Robust) â”€â”€
    # Direct model estimate (reward prediction)
    reward_model = train_lgbm(X_tr, y_tr, X_te, y_te,
                              dict(BASE_PARAMS, n_estimators=100), spw)
    mu_hat = reward_model.predict_proba(X_te)[:, 1]

    # DR = E[mu_hat(x)] + E[w * (r - mu_hat(x))]
    dr_direct = np.mean(mu_hat * target_prob + (1 - target_prob) * 0)
    dr_correction = np.mean(w_clipped * (rewards - mu_hat * logged_action))
    dr_estimate = dr_direct + dr_correction
    dr_var = np.var(mu_hat + w_clipped * (rewards - mu_hat * logged_action)) / len(rewards)

    # Confidence intervals (95%)
    z = 1.96
    results = {}
    for name, est, var in [("IPS", ips_estimate, ips_var),
                           ("SNIPS", snips_estimate, snips_var),
                           ("DR", dr_estimate, dr_var)]:
        se = np.sqrt(max(var, 1e-12))
        ci_low = est - z * se
        ci_high = est + z * se
        results[name] = {"estimate": round(est, 6), "variance": round(var, 8),
                         "se": round(se, 6), "ci_95": [round(ci_low, 6), round(ci_high, 6)]}
        logger.info(f"  {name:<8s} Estimate={est:.6f}  Var={var:.8f}  SE={se:.6f}  "
                    f"95% CI=[{ci_low:.6f}, {ci_high:.6f}]")

    logger.info(f"\n  DR has lowest variance: {dr_var:.8f} vs IPS: {ips_var:.8f}")
    logger.info(f"  Variance reduction: {(1 - dr_var/max(ips_var, 1e-10))*100:.1f}%")

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. MULTI-OBJECTIVE OPTIMIZATION (Lagrangian)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def task2_multi_objective(X, y_ctr, y_cvr, prices):
    logger.info("=" * 65)
    logger.info("TASK 2: MULTI-OBJECTIVE OPTIMIZATION (Lagrangian)")
    logger.info("=" * 65)

    n = X.shape[0]
    n_train = int(n * 0.7)
    X_tr, y_tr = X[:n_train], y_ctr[:n_train]
    X_te, y_te = X[n_train:], y_ctr[n_train:]
    y_click_te = y_ctr[n_train:]
    y_conv_te = y_cvr[n_train:]
    prices_te = prices[n_train:]
    spw = (1.0 - y_tr.mean()) / (y_tr.mean() + 1e-6)

    model = train_lgbm(X_tr, y_tr, X_te, y_te, BASE_PARAMS, spw)
    pCTR = model.predict_proba(X_te)[:, 1]
    ev = pCTR * config.value_click
    BUDGET = float(prices_te.sum()) * 0.8

    # Constraints
    TARGET_CPA = 200.0     # Max cost per acquisition
    TARGET_ROI = 0.85      # Min ROI
    UTIL_FLOOR = 0.80      # Min budget utilization
    MIN_WINS = int(len(prices_te) * 0.10)  # Min 10% win rate

    # Lagrangian multipliers
    lambda_cpa = 0.0    # Penalty for CPA violation
    lambda_roi = 0.0    # Penalty for ROI violation  
    lambda_util = 0.0   # Penalty for under-utilization
    lambda_vol = 0.0    # Penalty for low volume

    lr = 0.01  # Learning rate for multiplier updates

    logger.info(f"  Constraints: CPAâ‰¤{TARGET_CPA}, ROIâ‰¥{TARGET_ROI}, Utilâ‰¥{UTIL_FLOOR}, MinWinsâ‰¥{MIN_WINS}")
    logger.info(f"\n  {'Iter':<6} {'Î»_CPA':<8} {'Î»_ROI':<8} {'Î»_Util':<8} {'Î»_Vol':<8} "
                f"{'ROI':<8} {'CPA':<8} {'Util':<8} {'Wins':<8} {'Convs':<6}")

    trade_off = []

    for iteration in range(20):
        # Adjusted bid: EV + Lagrangian terms
        # Higher lambda_util â†’ more aggressive bidding
        # Higher lambda_cpa â†’ more conservative bidding
        bid_adj = ev * (1 + lambda_util + lambda_vol) / (1 + lambda_cpa + lambda_roi)

        # Simulate auction
        spend, wins, clicks, convs = 0.0, 0, 0, 0
        for i in range(len(prices_te)):
            mp = prices_te[i]
            if spend + mp <= BUDGET and bid_adj[i] >= mp:
                spend += mp
                wins += 1
                if y_click_te[i]: clicks += 1
                if y_conv_te[i]: convs += 1

        roi = calc_roi(clicks, convs, spend)
        cpa = spend / max(convs, 1)
        util = spend / BUDGET
        n_te = len(prices_te)

        trade_off.append({
            "iter": iteration, "roi": round(roi, 4), "cpa": round(cpa, 2),
            "util": round(util, 4), "wins": wins, "convs": convs,
            "lambda_cpa": round(lambda_cpa, 4), "lambda_roi": round(lambda_roi, 4),
            "lambda_util": round(lambda_util, 4), "lambda_vol": round(lambda_vol, 4),
        })

        logger.info(f"  {iteration:<6} {lambda_cpa:<8.4f} {lambda_roi:<8.4f} {lambda_util:<8.4f} {lambda_vol:<8.4f} "
                    f"{roi:<8.4f} {cpa:<8.1f} {util:<8.4f} {wins:<8} {convs:<6}")

        # Update multipliers (sub-gradient ascent)
        lambda_cpa = max(0, lambda_cpa + lr * (cpa - TARGET_CPA) / TARGET_CPA)
        lambda_roi = max(0, lambda_roi + lr * (TARGET_ROI - roi))
        lambda_util = max(0, lambda_util + lr * (UTIL_FLOOR - util))
        lambda_vol = max(0, lambda_vol + lr * (MIN_WINS - wins) / max(MIN_WINS, 1))

    best = max(trade_off, key=lambda x: x['roi'] if x['util'] >= 0.3 else 0)
    logger.info(f"\n  Best feasible: Iter={best['iter']}, ROI={best['roi']}, CPA={best['cpa']}, Util={best['util']}")

    return {"trade_off": trade_off, "best": best}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ONLINE LEARNING SIMULATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def task3_online_learning(X, y_ctr, y_cvr, prices):
    logger.info("=" * 65)
    logger.info("TASK 3: ONLINE LEARNING SIMULATION (Rolling Retraining)")
    logger.info("=" * 65)

    n = X.shape[0]
    # Split into 5 "days"
    day_size = n // 5
    days = []
    for d in range(5):
        s = d * day_size
        e = min(s + day_size, n)
        days.append((s, e))

    spw = (1.0 - y_ctr.mean()) / (y_ctr.mean() + 1e-6)
    results = []
    prev_model = None

    logger.info(f"  Simulating {len(days)} days, {day_size} impressions each")
    logger.info(f"\n  {'Day':<6} {'Mode':<20} {'AUC':<8} {'ROI':<8} {'WinRate':<10} {'Drift'}")

    for day_idx, (day_start, day_end) in enumerate(days):
        day_label = f"Day {day_idx + 1}"

        # Training data: all data up to this day
        train_end = day_start
        if train_end < 500:
            # Not enough train data for day 1 â€” use first 70%
            train_end = int(n * 0.5)

        X_tr = X[:train_end]
        y_tr = y_ctr[:train_end]
        X_day = X[day_start:day_end]
        y_day_ctr = y_ctr[day_start:day_end]
        y_day_cvr = y_cvr[day_start:day_end]
        prices_day = prices[day_start:day_end]

        # Validation: last 20% of training
        val_start = int(train_end * 0.8)
        X_val = X[val_start:train_end]
        y_val = y_ctr[val_start:train_end]

        if X_tr.shape[0] < 100 or X_val.shape[0] < 50:
            continue

        # Inject drift on day 4
        drift_applied = ""
        if day_idx == 3:
            # Simulate distribution shift: flip 10% of labels
            flip_n = int(len(y_day_ctr) * 0.10)
            flip_idx = np.random.choice(len(y_day_ctr), flip_n, replace=False)
            y_day_ctr_mod = y_day_ctr.copy()
            y_day_ctr_mod[flip_idx] = 1 - y_day_ctr_mod[flip_idx]
            drift_applied = "âš ï¸ 10% label flip"
        else:
            y_day_ctr_mod = y_day_ctr

        # Train model
        if day_idx == 0 or prev_model is None:
            # Cold start
            mode = "Cold Start"
            model = train_lgbm(X_tr, y_tr, X_val, y_val, BASE_PARAMS, spw)
        else:
            # Warm start (incremental)
            mode = "Warm Start"
            try:
                warm_params = dict(BASE_PARAMS)
                warm_params["n_estimators"] = 100  # Fewer trees for incremental
                model = train_lgbm(X_tr, y_tr, X_val, y_val, warm_params, spw,
                                   init_model=prev_model.booster_)
            except Exception:
                model = train_lgbm(X_tr, y_tr, X_val, y_val, BASE_PARAMS, spw)
                mode = "Full Retrain"

        prev_model = model

        # Evaluate on this day's data
        pCTR = model.predict_proba(X_day)[:, 1]
        auc = auc_safe(y_day_ctr_mod, pCTR)

        # ROI simulation
        ev = pCTR * config.value_click
        ev_thresh = np.percentile(ev, 70)
        BUDGET = float(prices_day.sum()) * 0.8
        spend, wins, clicks, convs = 0.0, 0, 0, 0
        for i in range(len(prices_day)):
            mp = prices_day[i]
            if ev[i] >= ev_thresh and spend + mp <= BUDGET and ev[i] >= mp:
                spend += mp
                wins += 1
                if y_day_ctr_mod[i]: clicks += 1
                if y_day_cvr[i]: convs += 1
        roi = calc_roi(clicks, convs, spend)
        win_rate = wins / max(len(prices_day), 1)

        results.append({
            "day": day_idx + 1, "mode": mode, "auc": round(auc, 4),
            "roi": round(roi, 4), "win_rate": round(win_rate, 4),
            "drift": drift_applied,
        })

        logger.info(f"  {day_label:<6} {mode:<20} {auc:<8.4f} {roi:<8.4f} {win_rate:<10.4f} {drift_applied}")

    # Recovery check
    if len(results) >= 5:
        day4_auc = results[3]['auc']
        day5_auc = results[4]['auc']
        recovery = day5_auc - day4_auc
        logger.info(f"\n  Drift Recovery: Day4 AUC={day4_auc:.4f} â†’ Day5 AUC={day5_auc:.4f} (Î”={recovery:+.4f})")

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. COMPETITOR MODELING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def task4_competitor_modeling(X, y_ctr, y_cvr, prices):
    logger.info("=" * 65)
    logger.info("TASK 4: COMPETITOR MODELING (Win-Prob + Bid Shading)")
    logger.info("=" * 65)

    n = X.shape[0]
    n_train = int(n * 0.7)
    X_tr, y_tr = X[:n_train], y_ctr[:n_train]
    X_te, y_te = X[n_train:], y_ctr[n_train:]
    y_click_te = y_ctr[n_train:]
    y_conv_te = y_cvr[n_train:]
    prices_te = prices[n_train:]
    spw = (1.0 - y_tr.mean()) / (y_tr.mean() + 1e-6)

    model = train_lgbm(X_tr, y_tr, X_te, y_te, BASE_PARAMS, spw)
    pCTR = model.predict_proba(X_te)[:, 1]
    ev = pCTR * config.value_click
    BUDGET = float(prices_te.sum()) * 0.8

    # Simulate competitor bids (log-normal distribution)
    mu_comp = np.log(prices_te.mean())
    sigma_comp = 0.5
    competitor_bids = np.random.lognormal(mu_comp, sigma_comp, len(prices_te))

    # Win probability model: P(win | bid) = CDF of competitor distribution at bid level
    def win_prob(bid):
        """Probability of winning given our bid and competitor distribution."""
        return sp_stats.lognorm.cdf(bid, s=sigma_comp, scale=np.exp(mu_comp))

    # â”€â”€ Strategy 1: Naive bidding (bid = EV) â”€â”€
    naive_spend, naive_wins, naive_clicks, naive_convs = 0.0, 0, 0, 0
    for i in range(len(prices_te)):
        bid = ev[i]
        # Win if bid > max competitor bid (simplified)
        if bid >= competitor_bids[i] and naive_spend + prices_te[i] <= BUDGET:
            naive_spend += prices_te[i]
            naive_wins += 1
            if y_click_te[i]: naive_clicks += 1
            if y_conv_te[i]: naive_convs += 1

    naive_roi = calc_roi(naive_clicks, naive_convs, naive_spend)

    # â”€â”€ Strategy 2: Win-prob bid shading â”€â”€
    # Optimal bid: maximize EV Ã— P(win|bid) - bid Ã— P(win|bid)
    # Shade bid to where marginal win-prob Ã— surplus is maximized
    shaded_spend, shaded_wins, shaded_clicks, shaded_convs = 0.0, 0, 0, 0
    for i in range(len(prices_te)):
        # Binary search for optimal shaded bid
        best_bid = 0
        best_surplus = -1
        for shade in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
            bid = ev[i] * shade
            pw = win_prob(bid)
            surplus = pw * (ev[i] - bid)  # Expected surplus
            if surplus > best_surplus:
                best_surplus = surplus
                best_bid = bid

        if best_bid >= competitor_bids[i] and shaded_spend + prices_te[i] <= BUDGET:
            shaded_spend += prices_te[i]
            shaded_wins += 1
            if y_click_te[i]: shaded_clicks += 1
            if y_conv_te[i]: shaded_convs += 1

    shaded_roi = calc_roi(shaded_clicks, shaded_convs, shaded_spend)

    # â”€â”€ Strategy 3: Dynamic competitor shift (aggressive competitors in 2nd half) â”€â”€
    comp_bids_shift = competitor_bids.copy()
    half = len(comp_bids_shift) // 2
    comp_bids_shift[half:] *= 1.5  # Competitors get 50% more aggressive

    dynamic_spend, dynamic_wins, dynamic_clicks, dynamic_convs = 0.0, 0, 0, 0
    for i in range(len(prices_te)):
        bid = ev[i] * 0.8  # Use shading
        if bid >= comp_bids_shift[i] and dynamic_spend + prices_te[i] <= BUDGET:
            dynamic_spend += prices_te[i]
            dynamic_wins += 1
            if y_click_te[i]: dynamic_clicks += 1
            if y_conv_te[i]: dynamic_convs += 1

    dynamic_roi = calc_roi(dynamic_clicks, dynamic_convs, dynamic_spend)

    n_te = len(prices_te)
    logger.info(f"\n  {'Strategy':<30s} {'Wins':<8} {'Clicks':<8} {'Convs':<8} {'ROI':<10} {'WinRate'}")
    logger.info(f"  {'Naive (bid=EV)':<30s} {naive_wins:<8} {naive_clicks:<8} {naive_convs:<8} {naive_roi:<10.4f} {naive_wins/n_te:.4f}")
    logger.info(f"  {'Win-Prob Shading':<30s} {shaded_wins:<8} {shaded_clicks:<8} {shaded_convs:<8} {shaded_roi:<10.4f} {shaded_wins/n_te:.4f}")
    logger.info(f"  {'Shading + Comp Shift':<30s} {dynamic_wins:<8} {dynamic_clicks:<8} {dynamic_convs:<8} {dynamic_roi:<10.4f} {dynamic_wins/n_te:.4f}")

    roi_improvement = (shaded_roi - naive_roi) / max(naive_roi, 0.001) * 100
    logger.info(f"\n  Win-Prob Shading ROI Lift vs Naive: {roi_improvement:+.1f}%")
    logger.info(f"  Competitor distribution: LogNormal(Î¼={mu_comp:.2f}, Ïƒ={sigma_comp})")

    return {
        "naive": {"roi": round(naive_roi, 4), "wins": naive_wins, "win_rate": round(naive_wins/n_te, 4)},
        "shaded": {"roi": round(shaded_roi, 4), "wins": shaded_wins, "win_rate": round(shaded_wins/n_te, 4)},
        "dynamic": {"roi": round(dynamic_roi, 4), "wins": dynamic_wins, "win_rate": round(dynamic_wins/n_te, 4)},
        "shading_lift": round(roi_improvement, 1),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. SHADOW DEPLOYMENT HARNESS (24h Campaign)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def task5_shadow_deployment(X, y_ctr, y_cvr, prices):
    logger.info("=" * 65)
    logger.info("TASK 5: SHADOW DEPLOYMENT (24h Campaign Simulation)")
    logger.info("=" * 65)

    n = X.shape[0]
    n_train = int(n * 0.7)
    X_tr, y_tr = X[:n_train], y_ctr[:n_train]
    X_te, y_te = X[n_train:], y_ctr[n_train:]
    y_click_te = y_ctr[n_train:]
    y_conv_te = y_cvr[n_train:]
    prices_te = prices[n_train:]
    spw = (1.0 - y_tr.mean()) / (y_tr.mean() + 1e-6)

    model = train_lgbm(X_tr, y_tr, X_te, y_te, BASE_PARAMS, spw)
    pCTR = model.predict_proba(X_te)[:, 1]
    ev = pCTR * config.value_click

    n_te = len(prices_te)
    TOTAL_BUDGET = float(prices_te.sum()) * 0.8
    HOURS = 24
    impressions_per_hour = n_te // HOURS

    # PID Pacing
    hourly_budget = TOTAL_BUDGET / HOURS
    cumulative_target = 0.0
    cumulative_spend = 0.0
    pacing_alpha = 1.0

    # Drift detection window
    pred_window = deque(maxlen=2000)
    psi_alerts = []

    # Tracking
    hourly_data = []

    logger.info(f"  Total Budget: {TOTAL_BUDGET:.0f}, Hourly Target: {hourly_budget:.0f}")
    logger.info(f"  Impressions: {n_te}, Per Hour: {impressions_per_hour}")
    logger.info(f"\n  {'Hour':<6} {'Spend':<10} {'CumSpend':<12} {'CumUtil':<10} {'Wins':<6} "
                f"{'Clicks':<8} {'ROI':<8} {'PacingAlpha':<12} {'Drift'}")

    total_wins, total_clicks, total_convs = 0, 0, 0
    ev_thresh = np.percentile(ev, 70)

    for hour in range(HOURS):
        h_start = hour * impressions_per_hour
        h_end = min(h_start + impressions_per_hour, n_te)

        cumulative_target += hourly_budget
        hour_spend = 0.0
        hour_wins = 0
        hour_clicks = 0
        hour_convs = 0

        for i in range(h_start, h_end):
            mp = prices_te[i]
            bid = ev[i] * pacing_alpha

            # EV gate
            if ev[i] < ev_thresh:
                continue

            # Shadow: log what would happen
            if bid >= mp and cumulative_spend + mp <= TOTAL_BUDGET:
                cumulative_spend += mp
                hour_spend += mp
                hour_wins += 1
                total_wins += 1
                if y_click_te[i]:
                    hour_clicks += 1
                    total_clicks += 1
                if y_conv_te[i]:
                    hour_convs += 1
                    total_convs += 1

            # Track predictions for drift
            pred_window.append(pCTR[i])

        # PID pacing update
        error = cumulative_target - cumulative_spend
        pacing_alpha = max(0.3, min(2.0, pacing_alpha + 0.01 * error / max(hourly_budget, 1)))

        # PSI drift check (compare first vs recent predictions)
        drift_alert = ""
        if len(pred_window) >= 2000:
            first_half = list(pred_window)[:1000]
            second_half = list(pred_window)[1000:]
            # Simple PSI approximation
            bins = np.linspace(0, 1, 11)
            h1 = np.histogram(first_half, bins=bins, density=True)[0] + 1e-6
            h2 = np.histogram(second_half, bins=bins, density=True)[0] + 1e-6
            h1 = h1 / h1.sum()
            h2 = h2 / h2.sum()
            psi = np.sum((h2 - h1) * np.log(h2 / h1))
            if psi > 0.2:
                drift_alert = f"âš ï¸ PSI={psi:.3f}"
                psi_alerts.append({"hour": hour, "psi": round(psi, 4)})

        cum_util = cumulative_spend / TOTAL_BUDGET
        cum_roi = calc_roi(total_clicks, total_convs, cumulative_spend)

        hourly_data.append({
            "hour": hour, "spend": round(hour_spend, 2), "cum_spend": round(cumulative_spend, 2),
            "cum_util": round(cum_util, 4), "wins": hour_wins, "clicks": hour_clicks,
            "roi": round(cum_roi, 4), "pacing_alpha": round(pacing_alpha, 4),
            "drift": drift_alert,
        })

        logger.info(f"  H{hour:<5} {hour_spend:<10.0f} {cumulative_spend:<12.0f} {cum_util:<10.4f} "
                    f"{hour_wins:<6} {hour_clicks:<8} {cum_roi:<8.4f} {pacing_alpha:<12.4f} {drift_alert}")

    final_util = cumulative_spend / TOTAL_BUDGET
    final_roi = calc_roi(total_clicks, total_convs, cumulative_spend)

    logger.info("\n  24h Summary:")
    logger.info(f"    Total Spend: {cumulative_spend:.0f} / {TOTAL_BUDGET:.0f} ({final_util:.1%} utilized)")
    logger.info(f"    Total Wins: {total_wins}, Clicks: {total_clicks}, Convs: {total_convs}")
    logger.info(f"    Final ROI: {final_roi:.4f}")
    logger.info(f"    PSI Alerts: {len(psi_alerts)}")

    return {
        "hourly": hourly_data,
        "final_util": round(final_util, 4),
        "final_roi": round(final_roi, 4),
        "total_wins": total_wins,
        "total_clicks": total_clicks,
        "total_convs": total_convs,
        "psi_alerts": psi_alerts,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    logger.info("ğŸš€ PHASE 10: INSTITUTIONAL-GRADE OPTIMIZATION")
    logger.info("=" * 65)

    logger.info("Loading data...")
    X, y_ctr, y_cvr, prices, scaler, stats, top_k_maps, full_df = prepare_data()
    logger.info(f"Dataset: {X.shape[0]} samples, {X.shape[1]} features")

    # Task 1
    cf_results = task1_counterfactual(X, y_ctr, prices)

    # Task 2
    mo_results = task2_multi_objective(X, y_ctr, y_cvr, prices)

    # Task 3
    ol_results = task3_online_learning(X, y_ctr, y_cvr, prices)

    # Task 4
    comp_results = task4_competitor_modeling(X, y_ctr, y_cvr, prices)

    # Task 5
    shadow_results = task5_shadow_deployment(X, y_ctr, y_cvr, prices)

    # Summary
    logger.info("\n" + "=" * 65)
    logger.info("PHASE 10 COMPLETE â€” All institutional-grade tasks executed")
    logger.info("=" * 65)
    logger.info(f"  DR Estimate: {cf_results['DR']['estimate']:.6f} (lowest variance)")
    logger.info(f"  Best Lagrangian ROI: {mo_results['best']['roi']:.4f}")
    logger.info(f"  Online Learning Days: {len(ol_results)}")
    logger.info(f"  Bid Shading Lift: {comp_results['shading_lift']:+.1f}%")
    logger.info(f"  Shadow 24h ROI: {shadow_results['final_roi']:.4f}, Util: {shadow_results['final_util']:.1%}")

    return {
        "counterfactual": cf_results,
        "multi_objective": mo_results,
        "online_learning": ol_results,
        "competitor": comp_results,
        "shadow": shadow_results,
    }


if __name__ == "__main__":
    results = main()
